{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5zMcDgM2-Lp"
      },
      "source": [
        "#<font color=yellow>16º Algoritmo Para 7 Espécies de Flores\n",
        "\n",
        "##Para resolver o problema de exceder a memória RAM no Google Colab, podemos adotar algumas estratégias para otimizar o uso de memória durante o carregamento e processamento das imagens:\n",
        "\n",
        "* Carregar imagens em lotes: Em vez de carregar todas as imagens de uma vez na memória, vamos carregar as imagens em lotes durante o treinamento.\n",
        "\n",
        "* Ajustar o tamanho do lote: Definiremos um tamanho de lote adequado que balanceie entre uso de memória e tempo de treinamento.\n",
        "\n",
        "* Utilizar ImageDataGenerator: Utilizaremos o ImageDataGenerator do Keras para carregar e processar as imagens dinamicamente durante o treinamento.\n",
        "\n",
        "##Este código utiliza ImageDataGenerator para carregar as imagens em lotes durante o treinamento e validação, o que ajuda a evitar o consumo excessivo de memória. O treinamento deve agora ser mais eficiente em termos de memória, permitindo que o código seja executado no Google Colab sem problemas de memória."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G2IAF0NT0k3H",
        "outputId": "61dd32e0-bd67-491a-bb47-8b6bcc699c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 8400 images belonging to 7 classes.\n",
            "Found 2520 images belonging to 7 classes.\n",
            "Epoch 1/50\n",
            "262/262 [==============================] - 153s 435ms/step - loss: 1.5756 - accuracy: 0.4524 - val_loss: 3.1838 - val_accuracy: 0.1623\n",
            "Epoch 2/50\n",
            "262/262 [==============================] - 100s 382ms/step - loss: 1.1461 - accuracy: 0.5890 - val_loss: 1.2678 - val_accuracy: 0.5717\n",
            "Epoch 3/50\n",
            "262/262 [==============================] - 84s 321ms/step - loss: 0.9718 - accuracy: 0.6642 - val_loss: 1.1631 - val_accuracy: 0.6430\n",
            "Epoch 4/50\n",
            "262/262 [==============================] - 125s 476ms/step - loss: 0.8702 - accuracy: 0.6918 - val_loss: 1.7569 - val_accuracy: 0.5477\n",
            "Epoch 5/50\n",
            "262/262 [==============================] - 123s 467ms/step - loss: 0.8370 - accuracy: 0.7100 - val_loss: 1.1593 - val_accuracy: 0.6571\n",
            "Epoch 6/50\n",
            "262/262 [==============================] - 149s 569ms/step - loss: 0.9200 - accuracy: 0.6819 - val_loss: 1.6419 - val_accuracy: 0.4876\n",
            "Epoch 7/50\n",
            "262/262 [==============================] - 92s 348ms/step - loss: 0.8191 - accuracy: 0.7141 - val_loss: 1.0258 - val_accuracy: 0.6362\n",
            "Epoch 8/50\n",
            "262/262 [==============================] - 99s 378ms/step - loss: 0.7000 - accuracy: 0.7609 - val_loss: 0.9385 - val_accuracy: 0.6719\n",
            "Epoch 9/50\n",
            "262/262 [==============================] - 82s 311ms/step - loss: 0.6822 - accuracy: 0.7640 - val_loss: 0.9844 - val_accuracy: 0.6635\n",
            "Epoch 10/50\n",
            "262/262 [==============================] - 100s 381ms/step - loss: 0.6424 - accuracy: 0.7820 - val_loss: 0.8797 - val_accuracy: 0.7115\n",
            "Epoch 11/50\n",
            "262/262 [==============================] - 99s 378ms/step - loss: 0.5426 - accuracy: 0.8142 - val_loss: 1.0589 - val_accuracy: 0.6707\n",
            "Epoch 12/50\n",
            "262/262 [==============================] - 80s 306ms/step - loss: 0.4842 - accuracy: 0.8370 - val_loss: 0.8724 - val_accuracy: 0.7276\n",
            "Epoch 13/50\n",
            "262/262 [==============================] - 81s 308ms/step - loss: 0.4421 - accuracy: 0.8528 - val_loss: 1.0424 - val_accuracy: 0.6831\n",
            "Epoch 14/50\n",
            "262/262 [==============================] - 81s 309ms/step - loss: 0.4592 - accuracy: 0.8458 - val_loss: 0.9264 - val_accuracy: 0.7115\n",
            "Epoch 15/50\n",
            "262/262 [==============================] - 98s 376ms/step - loss: 0.5640 - accuracy: 0.8082 - val_loss: 0.8749 - val_accuracy: 0.7011\n",
            "Epoch 16/50\n",
            "262/262 [==============================] - 98s 375ms/step - loss: 0.6537 - accuracy: 0.7751 - val_loss: 0.9560 - val_accuracy: 0.6907\n",
            "Epoch 17/50\n",
            "262/262 [==============================] - 99s 377ms/step - loss: 0.5619 - accuracy: 0.8016 - val_loss: 0.9208 - val_accuracy: 0.6883\n",
            "Epoch 18/50\n",
            "262/262 [==============================] - 81s 308ms/step - loss: 0.4747 - accuracy: 0.8405 - val_loss: 0.7962 - val_accuracy: 0.7332\n",
            "Epoch 19/50\n",
            "262/262 [==============================] - 100s 380ms/step - loss: 0.3229 - accuracy: 0.8893 - val_loss: 0.9127 - val_accuracy: 0.7071\n",
            "Epoch 20/50\n",
            "262/262 [==============================] - 82s 313ms/step - loss: 0.3171 - accuracy: 0.8933 - val_loss: 0.9968 - val_accuracy: 0.7103\n",
            "Epoch 21/50\n",
            "262/262 [==============================] - 82s 314ms/step - loss: 0.4438 - accuracy: 0.8489 - val_loss: 1.3564 - val_accuracy: 0.6062\n",
            "Epoch 22/50\n",
            "262/262 [==============================] - 100s 383ms/step - loss: 0.3003 - accuracy: 0.8967 - val_loss: 0.9644 - val_accuracy: 0.7332\n",
            "Epoch 23/50\n",
            "262/262 [==============================] - 80s 305ms/step - loss: 0.2288 - accuracy: 0.9190 - val_loss: 1.0022 - val_accuracy: 0.7119\n",
            "Epoch 24/50\n",
            "262/262 [==============================] - 81s 310ms/step - loss: 0.2444 - accuracy: 0.9175 - val_loss: 0.9624 - val_accuracy: 0.7288\n",
            "Epoch 25/50\n",
            "262/262 [==============================] - 98s 374ms/step - loss: 0.4997 - accuracy: 0.8381 - val_loss: 2.1462 - val_accuracy: 0.4307\n",
            "Epoch 26/50\n",
            "262/262 [==============================] - 81s 311ms/step - loss: 0.5206 - accuracy: 0.8234 - val_loss: 1.4772 - val_accuracy: 0.5553\n",
            "Epoch 27/50\n",
            "262/262 [==============================] - 81s 309ms/step - loss: 0.5777 - accuracy: 0.7973 - val_loss: 2.2478 - val_accuracy: 0.4431\n",
            "Epoch 28/50\n",
            "262/262 [==============================] - 81s 309ms/step - loss: 0.4860 - accuracy: 0.8298 - val_loss: 0.9546 - val_accuracy: 0.7003\n",
            "Epoch 29/50\n",
            "262/262 [==============================] - 99s 377ms/step - loss: 0.3728 - accuracy: 0.8756 - val_loss: 1.0262 - val_accuracy: 0.7111\n",
            "Epoch 30/50\n",
            "262/262 [==============================] - 80s 305ms/step - loss: 0.2191 - accuracy: 0.9290 - val_loss: 0.9213 - val_accuracy: 0.7348\n",
            "Epoch 31/50\n",
            "262/262 [==============================] - 97s 368ms/step - loss: 0.1918 - accuracy: 0.9351 - val_loss: 1.0270 - val_accuracy: 0.7208\n",
            "Epoch 32/50\n",
            "262/262 [==============================] - 99s 376ms/step - loss: 0.1432 - accuracy: 0.9520 - val_loss: 0.9566 - val_accuracy: 0.7516\n",
            "Epoch 33/50\n",
            "262/262 [==============================] - 99s 377ms/step - loss: 0.1219 - accuracy: 0.9596 - val_loss: 1.2424 - val_accuracy: 0.6831\n",
            "Epoch 34/50\n",
            "262/262 [==============================] - 80s 304ms/step - loss: 0.2460 - accuracy: 0.9153 - val_loss: 3.2157 - val_accuracy: 0.3586\n",
            "Epoch 35/50\n",
            "262/262 [==============================] - 80s 305ms/step - loss: 0.4630 - accuracy: 0.8489 - val_loss: 1.0013 - val_accuracy: 0.7208\n",
            "Epoch 36/50\n",
            "262/262 [==============================] - 79s 302ms/step - loss: 0.2043 - accuracy: 0.9295 - val_loss: 0.9795 - val_accuracy: 0.7400\n",
            "Epoch 37/50\n",
            "262/262 [==============================] - 79s 302ms/step - loss: 0.1352 - accuracy: 0.9594 - val_loss: 1.0584 - val_accuracy: 0.7424\n",
            "Epoch 38/50\n",
            "262/262 [==============================] - 97s 371ms/step - loss: 0.0996 - accuracy: 0.9656 - val_loss: 1.1340 - val_accuracy: 0.7316\n",
            "Epoch 39/50\n",
            "262/262 [==============================] - 97s 369ms/step - loss: 0.0843 - accuracy: 0.9723 - val_loss: 1.1457 - val_accuracy: 0.7384\n",
            "Epoch 40/50\n",
            "262/262 [==============================] - 80s 306ms/step - loss: 0.0673 - accuracy: 0.9769 - val_loss: 1.1321 - val_accuracy: 0.7480\n",
            "Epoch 41/50\n",
            "262/262 [==============================] - 79s 303ms/step - loss: 0.0631 - accuracy: 0.9780 - val_loss: 1.1777 - val_accuracy: 0.7448\n",
            "Epoch 42/50\n",
            "262/262 [==============================] - 80s 305ms/step - loss: 0.0954 - accuracy: 0.9670 - val_loss: 1.2728 - val_accuracy: 0.7308\n",
            "Epoch 43/50\n",
            "262/262 [==============================] - 97s 372ms/step - loss: 0.0809 - accuracy: 0.9724 - val_loss: 1.2356 - val_accuracy: 0.7360\n",
            "Epoch 44/50\n",
            "262/262 [==============================] - 99s 377ms/step - loss: 0.1581 - accuracy: 0.9497 - val_loss: 1.8221 - val_accuracy: 0.5673\n",
            "Epoch 45/50\n",
            "262/262 [==============================] - 98s 376ms/step - loss: 0.6537 - accuracy: 0.7996 - val_loss: 1.0452 - val_accuracy: 0.6867\n",
            "Epoch 46/50\n",
            "262/262 [==============================] - 99s 379ms/step - loss: 0.3055 - accuracy: 0.8978 - val_loss: 1.5555 - val_accuracy: 0.5433\n",
            "Epoch 47/50\n",
            "262/262 [==============================] - 81s 308ms/step - loss: 0.2952 - accuracy: 0.8982 - val_loss: 1.0418 - val_accuracy: 0.7039\n",
            "Epoch 48/50\n",
            "262/262 [==============================] - 80s 306ms/step - loss: 0.1541 - accuracy: 0.9481 - val_loss: 1.0658 - val_accuracy: 0.7188\n",
            "Epoch 49/50\n",
            "262/262 [==============================] - 81s 308ms/step - loss: 0.1042 - accuracy: 0.9680 - val_loss: 1.7532 - val_accuracy: 0.5925\n",
            "Epoch 50/50\n",
            "262/262 [==============================] - 80s 307ms/step - loss: 0.1475 - accuracy: 0.9506 - val_loss: 1.2209 - val_accuracy: 0.7051\n",
            "79/79 [==============================] - 24s 310ms/step - loss: 1.2161 - accuracy: 0.7063\n",
            "Test Loss: 1.2160789966583252\n",
            "Test Accuracy: 0.7063491940498352\n",
            "78/78 [==============================] - 22s 282ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [2520, 2496]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b55156c7ecfa>\u001b[0m in \u001b[0;36m<cell line: 127>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Plotando gráfico de matriz de confusão para imagens de teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mconfusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m sns.heatmap(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2520, 2496]"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    balanced_accuracy_score,\n",
        "    precision_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve,\n",
        ")\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Montar o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Defina as dimensões das imagens\n",
        "img_height = 150\n",
        "img_width = 150\n",
        "num_classes = 7  # Sete classes para diferentes tipos de flores\n",
        "batch_size = 32  # Tamanho do lote\n",
        "\n",
        "# Dicionário de rótulos para cada tipo de flor\n",
        "flower_labels = {\n",
        "    \"Margarida\": 0,\n",
        "    \"Dente de Leão\": 1,\n",
        "    \"Rosa\": 2,\n",
        "    \"Girassol\": 3,\n",
        "    \"Tulipa\": 4,\n",
        "    \"Campânula\": 5,\n",
        "    \"Lótus\": 6,\n",
        "}\n",
        "\n",
        "# Função para criar o ImageDataGenerator\n",
        "def criar_data_generators(train_dir, val_dir, img_height, img_width, batch_size):\n",
        "    train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
        "    val_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='sparse'\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='sparse'\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "# Solicitar os caminhos das pastas de treino e validação\n",
        "train_dir = input(\"Digite o caminho da pasta de treino no Google Drive: \")\n",
        "val_dir = input(\"Digite o caminho da pasta de validação no Google Drive: \")\n",
        "\n",
        "# Criar os data generators\n",
        "train_generator, val_generator = criar_data_generators(train_dir, val_dir, img_height, img_width, batch_size)\n",
        "\n",
        "# ********************* IMPLEMENTAÇÃO DA REDE NEURAL ARTIFICIAL CNN LENET-5 ***********************\n",
        "\n",
        "# Construir o modelo LeNet-5 adaptado e aprimorado\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", input_shape=(img_height, img_width, 3), padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    keras.layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\", padding='same'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(256, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Treinar o modelo\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size, #Alteração para resolver o problema da memória\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "# ********************* FIM DO BLOCO DE CÓDIGO COM A REDE NEURAL CNN***************************************************\n",
        "\n",
        "# Avaliar o modelo nos dados de teste\n",
        "test_loss, test_accuracy = model.evaluate(val_generator)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Prever as classes para os dados de teste\n",
        "val_generator.reset()\n",
        "test_predictions = model.predict(val_generator, steps=val_generator.samples // batch_size, verbose=1) #Alteração para resolver o problema da memória\n",
        "test_predictions_classes = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Salvar o modelo na pasta de downloads\n",
        "model.save(\"/content/drive/My Drive/flower_classification_model.h5\")\n",
        "\n",
        "# Plotando gráfico de matriz de confusão para imagens de teste\n",
        "confusion = confusion_matrix(val_generator.classes, test_predictions_classes)\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(\n",
        "    confusion,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    cbar=False,\n",
        "    xticklabels=flower_labels.keys(),\n",
        "    yticklabels=flower_labels.keys(),\n",
        ")\n",
        "plt.xlabel(\"Predito\")\n",
        "plt.ylabel(\"Atual\")\n",
        "plt.title(\"Matriz de Confusão para Imagens de Teste\")\n",
        "plt.show()\n",
        "\n",
        "# Calculando métricas de validação para imagens de teste\n",
        "accuracy = accuracy_score(val_generator.classes, test_predictions_classes)\n",
        "balanced_accuracy = balanced_accuracy_score(val_generator.classes, test_predictions_classes)\n",
        "precision = precision_score(val_generator.classes, test_predictions_classes, average='weighted')\n",
        "f1 = f1_score(val_generator.classes, test_predictions_classes, average='weighted')\n",
        "recall = recall_score(val_generator.classes, test_predictions_classes, average='weighted')\n",
        "\n",
        "print(\"Acurácia para imagens de teste:\", accuracy)\n",
        "print(\"Acurácia Balanceada para imagens de teste:\", balanced_accuracy)\n",
        "print(\"Precisão para imagens de teste:\", precision)\n",
        "print(\"F1 Score para imagens de teste:\", f1)\n",
        "print(\"Recall para imagens de teste:\", recall)\n",
        "\n",
        "# Plotar gráficos de perda e acurácia\n",
        "plt.figure(figsize=(20, 12))\n",
        "\n",
        "# Gráfico de perda\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico de acurácia\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Transformar o problema em binário para uma abordagem \"one-vs-rest\"\n",
        "for class_index in range(num_classes):\n",
        "    print(f\"\\nClasse: {list(flower_labels.keys())[class_index]}\")\n",
        "\n",
        "    # Extrair rótulos binários para a classe atual\n",
        "    binary_labels = (val_generator.classes == class_index).astype(int)\n",
        "\n",
        "    # Calcular probabilidades previstas para a classe atual\n",
        "    predicted_probabilities = test_predictions[:, class_index]\n",
        "\n",
        "    # Curva Precision-Recall\n",
        "    precision, recall, thresholds = precision_recall_curve(binary_labels, predicted_probabilities)\n",
        "\n",
        "    # F1-score para diferentes limiares de probabilidade\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "\n",
        "    # Limiar que maximiza o F1-score\n",
        "    optimal_threshold_index = np.argmax(f1_scores)\n",
        "\n",
        "    # Limiar ótimo\n",
        "    optimal_threshold = thresholds[optimal_threshold_index]\n",
        "    optimal_precision = precision[optimal_threshold_index]\n",
        "    optimal_recall = recall[optimal_threshold_index]\n",
        "\n",
        "    print(\"Limiar ótimo que maximiza o F1-score (otimizando tanto a Precisão quanto o Recall):\", optimal_threshold)\n",
        "    print(\"Precisão ótima:\", optimal_precision)\n",
        "    print(\"Recall ótimo:\", optimal_recall)\n",
        "\n",
        "    # Plotando precision-recall curve\n",
        "    plt.figure(figsize=(18, 10))\n",
        "\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(recall, precision, marker='.', label=f'Precision-Recall Curve for {list(flower_labels.keys())[class_index]}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'Precision-Recall Curve for {list(flower_labels.keys())[class_index]}')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando precision & recall VS threshold\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(thresholds, precision[:-1], label='Precision', color='blue')\n",
        "    plt.plot(thresholds, recall[:-1], label='Recall', color='green')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title(f'Precision and Recall vs. Threshold for {list(flower_labels.keys())[class_index]}')\n",
        "    plt.axvline(x=optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold ({optimal_threshold:.3f})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando F1-score VS threshold\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.plot(thresholds, f1_scores[:-1], label='F1 Score', color='orange')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title(f'F1 Score vs. Threshold for {list(flower_labels.keys())[class_index]}')\n",
        "    plt.axvline(x=optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold ({optimal_threshold:.3f})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotando curva ROC e AUC\n",
        "    plt.subplot(2, 3, 4)\n",
        "    fpr, tpr, _ = roc_curve(binary_labels, predicted_probabilities)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'Receiver Operating Characteristic (ROC) Curve for {list(flower_labels.keys())[class_index]}')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Função para identificar e exibir novas imagens\n",
        "def identificar_e_exibir_imagens():\n",
        "    caminho_novas_imagens = input(\"Digite o caminho da pasta de novas imagens no Google Drive: \")\n",
        "\n",
        "    new_images = []\n",
        "    filenames = []\n",
        "    for filename in os.listdir(caminho_novas_imagens):\n",
        "        img_path = os.path.join(caminho_novas_imagens, filename)\n",
        "        img = Image.open(img_path).resize((img_height, img_width)).convert('RGB')\n",
        "        img_array = np.array(img)\n",
        "        new_images.append(img_array)\n",
        "        filenames.append(filename)\n",
        "\n",
        "    if not new_images:\n",
        "        print(\"Nenhuma imagem carregada.\")\n",
        "        return\n",
        "\n",
        "    new_images = np.array(new_images) / 255.0\n",
        "\n",
        "    predictions = model.predict(new_images)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    confidences = np.max(predictions, axis=1) * 100\n",
        "\n",
        "    num_cols = 5\n",
        "    num_rows = (len(new_images) + num_cols - 1) // num_cols  # Calcular o número de linhas necessário\n",
        "\n",
        "    plt.figure(figsize=(20, num_rows * 4))  # Ajustar o tamanho da figura\n",
        "\n",
        "    for i, (filename, predicted_class, confidence) in enumerate(zip(filenames, predicted_classes, confidences)):\n",
        "        plt.subplot(num_rows, num_cols, i + 1)\n",
        "        img = Image.open(os.path.join(caminho_novas_imagens, filename))\n",
        "        plt.imshow(img)\n",
        "        plt.title(f'{list(flower_labels.keys())[predicted_class]} ({confidence:.2f}%)')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Exibir quantidade de imagens novas classificadas e percentual de acurácia e precisão\n",
        "    total_images = len(new_images)\n",
        "    class_counts = np.bincount(predicted_classes, minlength=num_classes)\n",
        "    class_accuracies = [np.sum(predicted_classes == i) / total_images for i in range(num_classes)]\n",
        "    class_precisions = [precision_score(new_images, predicted_classes == i, average='weighted') for i in range(num_classes)]\n",
        "\n",
        "    table_data = {\n",
        "        \"Classe de Flor\": list(flower_labels.keys()),\n",
        "        \"Quantidade Classificada\": class_counts,\n",
        "        \"Acurácia (%)\": [acc * 100 for acc in class_accuracies],\n",
        "        \"Precisão (%)\": [prec * 100 for prec in class_precisions],\n",
        "    }\n",
        "    df = pd.DataFrame(table_data)\n",
        "\n",
        "    print(\"\\nTabela de Classificação de Imagens Novas:\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "# Função para o loop principal de identificação de imagens\n",
        "def main_loop():\n",
        "    while True:\n",
        "        identificar_e_exibir_imagens()\n",
        "        action = input(\"Digite 'novas' para identificar outras imagens ou 'sair' para terminar: \").lower()\n",
        "        if action == 'sair':\n",
        "            print(\"Encerrando o programa.\")\n",
        "            break\n",
        "\n",
        "# Chamar a função para identificar novas imagens\n",
        "main_loop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Porém mesmo com as implementações acima no código não foi possível resolver o problema do custo de memória."
      ],
      "metadata": {
        "id": "3CNRkF5RVBM7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}